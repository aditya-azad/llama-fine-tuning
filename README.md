# Llamathlete

This [Kaggle competition](https://www.kaggle.com/competitions/nyu-dl-fall-24-competition/overview) required us to fine-tune Llama 3.1 8B for the task of grading answers to math questions. The provided dataset contained the questions, answers, explanations for those answers, and whether the answer was correct. The challenge was difficult given the weakness of LLMs in mathematical reasoning and the time requirements for fine-tuning them. We addressed these by reasoning about and trialing different training hyper-parameters with the goal of enhancing our models' capacity to mathematically reason. Ultimately, we were able to raise ours models' accuracies from ~50% to ~84. While a significant improvement, we believe further improvements can be made with training on more data, trialing different prompts and parameter configurations, and adopting QLoRA.
