{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allan-jt/Llamathlete/blob/aditya/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70hrNJwhYMjR"
      },
      "source": [
        "# **Math Question Answer Verification Competition**\n",
        "\n",
        "This notebook is based on the starter code from the [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR).\n",
        "\n",
        "### Team Members:\n",
        "- **Aditya Azad**     (aa10878)\n",
        "- **Ching Huang**     (ch4802)\n",
        "- **Allan Thekkepeedika** (ajt444)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8dK32_gOZu"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CHBjw1hg1yKn"
      },
      "outputs": [],
      "source": [
        "COLLAB = 1\n",
        "KAGGLE = 0\n",
        "NOTEBOOK_ENV = COLLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA1lW9pzWwpk"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# This cell will take time\n",
        "\n",
        "if NOTEBOOK_ENV == COLLAB:\n",
        "  !pip install unsloth\n",
        "  !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "else:\n",
        "  !pip install pip3-autoremove\n",
        "  !pip-autoremove torch torchvision torchaudio -y\n",
        "  !pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "  !pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlMvIVDCDZ-B"
      },
      "outputs": [],
      "source": [
        "# If you're loading a model that you'e saved on google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlpjJOhtW7g3"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "max_seq_length = 1000 # The token size of our prompt doesn't exceed 500, so this is a safe value\n",
        "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GxOyBTkXJIG"
      },
      "outputs": [],
      "source": [
        "# Either start fresh with pretrained Llama or load your fine-tuned model\n",
        "# for further training and insert path in model_name\n",
        "\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVgabGjM8G1r"
      },
      "source": [
        "## Wrap model with LoRA adapters\n",
        "\n",
        "Note: This is only required if you're starting fresh with Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Is48lqR30z7"
      },
      "outputs": [],
      "source": [
        "# Configrations we experimented with; all others were default\n",
        "class LoRAConfig:\n",
        "  def __init__(self, r, lora_alpha, use_rslora=True):\n",
        "    self.r = r\n",
        "    self.lora_alpha = lora_alpha\n",
        "    self.use_rslora = use_rslora\n",
        "\n",
        "# Instantiate configurations\n",
        "LoRAConfig1 = LoRAConfig(r=16, lora_alpha=16, use_rslora=True)\n",
        "LoRAConfig2 = LoRAConfig(r=32, lora_alpha=32, use_rslora=True)\n",
        "LoRAConfig3 = LoRAConfig(r=32, lora_alpha=64, use_rslora=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy0iN0RJXMAX"
      },
      "outputs": [],
      "source": [
        "loraConfig = LoRAConfig3\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = loraConfig.r,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = loraConfig.lora_alpha,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = loraConfig.use_rslora,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNruHjDieGSS"
      },
      "source": [
        "## Download competition dataset and process with selected prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OMXJz4Z8jhJ"
      },
      "outputs": [],
      "source": [
        "# Load competition datasets and extract training data to create training and validation datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
        "train_data = dataset['train']\n",
        "\n",
        "# A large validation set can't fit in memory on Collab and Kaggle,\n",
        "# so we assign a small percentage (0.1%) of the training data for\n",
        "# validation because the size of the traiining data is 1,000,000.\n",
        "# train_size_percent = 0.999\n",
        "\n",
        "# dataset_size = len(dataset['train'])\n",
        "# train_size = round(train_size_percent * dataset_size)\n",
        "# val_size = dataset_size - train_size\n",
        "\n",
        "# train_data, val_data = torch.utils.data.random_split(dataset['train'], [train_size, val_size])\n",
        "# train_data = dataset['train'].select(train_data.indices)\n",
        "# val_data = dataset['train'].select(val_data.indices)\n",
        "\n",
        "# balance dataset\n",
        "false_indices = [\n",
        "    i for i, x in enumerate(train_data[\"is_correct\"]) if not x\n",
        "]\n",
        "indices_to_remove = np.random.choice(\n",
        "    false_indices, size=200000, replace=False\n",
        ")\n",
        "keep_mask = np.ones(len(train_data), dtype=bool)\n",
        "keep_mask[indices_to_remove] = False\n",
        "train_data = train_data.select(np.where(keep_mask)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DBpDwJA-bJ9K"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"You are a math grader tasked with evaluating whether a given answer to a math question is correct or not. Respond with 'True' if the answer is correct and 'False' if it is incorrect.\n",
        "\n",
        "Below are the Question, the given Answer, and the Explanation of the Answer.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "### Explanation:\n",
        "{}\n",
        "\n",
        "### Itâ€™s very important to grade the Answer accurately, so you must\n",
        "1.  Carefully read and understand the Question.\n",
        "2.  Review the given Answer and compare it against the Explanation provided.\n",
        "3.  If the Explanation correctly justifies the Answer, respond with 'True'.\n",
        "4.  If the Explanation is incorrect or does not logically support the Answer, respond with 'False'.\n",
        "\n",
        "### Grading ('True' or 'False'):\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    question = examples[\"question\"]\n",
        "    answer = examples[\"answer\"]\n",
        "    explanation = examples[\"solution\"]\n",
        "    output = examples[\"is_correct\"]\n",
        "\n",
        "    texts = []\n",
        "    for q, a, e, o in zip(question, answer, explanation, output):\n",
        "        # Must add EOS_TOKEN to prevent infinite generation\n",
        "        text = prompt.format(q, a, e, o) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEeHyA68-puB"
      },
      "outputs": [],
      "source": [
        "# Process the training and validation datasets and generate prompt for each datapoint\n",
        "train_dataset = train_data.map(formatting_prompts_func, batched = True)\n",
        "# val_dataset = val_data.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKBG7s8woNuo"
      },
      "outputs": [],
      "source": [
        "# Print a smaple training datapoint\n",
        "train_dataset['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CpKj5Na6qbC"
      },
      "outputs": [],
      "source": [
        "# Print a smaple validation datapoint\n",
        "# val_dataset['text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egSQOrCJeM7n"
      },
      "source": [
        "## Supervised Fine-tuning Trainer (SFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2oBKyYkh7-Mt"
      },
      "outputs": [],
      "source": [
        "# Configrations we experimented with; all others were default\n",
        "class SFTConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size,\n",
        "        accumulation_steps,\n",
        "        warmup_steps,\n",
        "        learning_rate,\n",
        "        lr_scheduler_type,\n",
        "        max_steps,\n",
        "        weight_decay\n",
        "    ):\n",
        "        self.batch_size = batch_size\n",
        "        self.accumulation_steps = accumulation_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lr_scheduler_type = lr_scheduler_type\n",
        "        self.max_steps = max_steps\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "# Define learning rates\n",
        "lr_slow, lr_medium, lr_fast = 1e-4, 2e-4, 3e-4\n",
        "\n",
        "# Create configuration instances\n",
        "SFTConfig1 = SFTConfig(\n",
        "    batch_size=4,\n",
        "    accumulation_steps=4,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=lr_slow,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=2000,\n",
        "    weight_decay=0.001\n",
        ")\n",
        "\n",
        "SFTConfig2 = SFTConfig(\n",
        "    batch_size=8,\n",
        "    accumulation_steps=4,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=lr_slow,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=1200,\n",
        "    weight_decay=0.001\n",
        ")\n",
        "\n",
        "SFTConfig3 = SFTConfig(\n",
        "    batch_size=8,\n",
        "    accumulation_steps=4,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=lr_slow,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_steps=3500,\n",
        "    weight_decay=0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INoSdVrEbO9Q"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "sftConfig = SFTConfig3\n",
        "training_args = TrainingArguments(\n",
        "#   Custom configurations\n",
        "    per_device_train_batch_size = sftConfig.batch_size,\n",
        "    gradient_accumulation_steps = sftConfig.accumulation_steps,\n",
        "    warmup_steps = sftConfig.max_steps,\n",
        "    max_steps = sftConfig.warmup_steps,\n",
        "    learning_rate = sftConfig.learning_rate,\n",
        "    lr_scheduler_type = sftConfig.lr_scheduler_type,\n",
        "    weight_decay = sftConfig.weight_decay,\n",
        "\n",
        "#   Default configurations\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"outputs\",\n",
        "    report_to = \"none\",\n",
        "\n",
        "#   Validation configrations\n",
        "    # per_device_eval_batch_size = 2,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 100,\n",
        "    # eval_accumulation_steps = 10,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    # eval_dataset = val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 4,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = training_args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WquBPTm4b-3z"
      },
      "outputs": [],
      "source": [
        "RESUME_TRAINING = False\n",
        "\n",
        "if RESUME_TRAINING:\n",
        "    trainer_stats = trainer.train(resume_from_checkpoint=True)\n",
        "else:\n",
        "    trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKt3vZoSeRvb"
      },
      "source": [
        "## Save model and training state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRiW2RQ0cWru"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"outputs\") # Local saving\n",
        "tokenizer.save_pretrained(\"outputs\")\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfnnGUNCB9OO"
      },
      "source": [
        "## Conducting inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SzYhOCLCCoZ"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNBQCPrVCKB1"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset['test']\n",
        "\n",
        "sample_ques = test_dataset['question']\n",
        "sample_ans = test_dataset['answer']\n",
        "sample_sol = test_dataset['solution']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEmuPzLMCMuM"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "sol = []\n",
        "\n",
        "# Prepare your input prompts\n",
        "input_prompts = []\n",
        "for i in range(len(sample_ques)):\n",
        "    input = prompt.format(\n",
        "        sample_ques[i], # question\n",
        "        sample_ans[i],  # given answer\n",
        "        sample_sol[i],  # explanation\n",
        "        \"\",             # output - leave this blank for generation\n",
        "    )\n",
        "    input_prompts.append(input)\n",
        "\n",
        "chunk_size = 16 # Divide your dataset into smaller chunks\n",
        "for i in range(0, len(input_prompts), chunk_size):\n",
        "    chunk = input_prompts[i:i + chunk_size]\n",
        "    inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=False, max_length= max_seq_length).to(\"cuda\")\n",
        "    input_shape = inputs['input_ids'].shape\n",
        "    input_token_len = input_shape[1] # 1 because of batch\n",
        "\n",
        "    # Tokenize and run inference for the chunk\n",
        "    outputs = model.generate(**inputs, max_new_tokens=1,use_cache=True)\n",
        "    response = tokenizer.batch_decode(outputs[:, input_token_len:], skip_special_tokens=True)\n",
        "    sol = sol + response\n",
        "\n",
        "    # Clear cache after each chunk to free up memory\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FanN9UHRCiJ5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LIpQEwkCjrQ"
      },
      "outputs": [],
      "source": [
        "ID = [i for i in range(len(sol))]\n",
        "is_correct = [s == 'True' for s in sol]\n",
        "dict = {'ID': ID, 'is_correct': is_correct}\n",
        "\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}